{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ADg67ezxU_th2aeC9AW6LlVxQm3E_sCW","timestamp":1733970562551},{"file_id":"1l7vhUUd49Uf_5BISstkbK3BZXBy9Jv-i","timestamp":1701265635372},{"file_id":"13YvgXhAijUSJpe1IdUZ-OvhOXdnJO6fl","timestamp":1669366077596},{"file_id":"1HEm5t8aD4Qua7u70GnCxJ1c8BZu4z5gP","timestamp":1669262631061},{"file_id":"1oUhePzHYmwsSxt0fDWL5JEhU7QRH1FVu","timestamp":1669252017215}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# BigData解析　-Pysparkを用いた演習-\n","\n","このバージョンはデータは乳ガンデータ（SKLearnよりDLしてCSV化）"],"metadata":{"id":"Azse1gvoNJA9"}},{"cell_type":"markdown","source":["# Pysparkのインストール"],"metadata":{"id":"zxWnkihHM36T"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"J_MG0UY0MrVE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733970682861,"user_tz":-540,"elapsed":10582,"user":{"displayName":"Utaya TAKAIE","userId":"10278607304609517554"}},"outputId":"f1696b8b-0591-4538-abee-2d4753800eb6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"]}],"source":["!pip install pyspark"]},{"cell_type":"markdown","source":["# Googleドライブとの連携"],"metadata":{"id":"4jz0OjlSM5I9"}},{"cell_type":"code","source":["from google import colab\n","colab.drive.mount('/content/gdrive')"],"metadata":{"id":"XSAciBOBRjdI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733970717864,"user_tz":-540,"elapsed":21278,"user":{"displayName":"Utaya TAKAIE","userId":"10278607304609517554"}},"outputId":"5eb28306-f1ae-4a96-8993-9bd507d9a9f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["#ディレクトリ設定\n","b_dir='gdrive/MyDrive/BigData2024/Data/'"],"metadata":{"id":"x64ZRuE0SNO_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# データ準備"],"metadata":{"id":"ubTcOvEyNUUq"}},{"cell_type":"code","source":["#import\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import *\n","\n","from pyspark.ml.feature import VectorAssembler  #特徴ベクトル変換"],"metadata":{"id":"2tBEQ5i6NWlI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#SparkSessionのインスタンス化\n","ss=(SparkSession.builder.appName(\"Classifier\").enableHiveSupport().getOrCreate())\n","\n","#スキーマ定義 これはデータによって異なる\n","\n","struct=StructType([\n","    StructField('num',StringType(),False), StructField('mean radius',DoubleType(),False),StructField('mean texture',DoubleType(),False),\n","    StructField('mean perimeter',DoubleType(),False),StructField('mean area',DoubleType(),False), StructField('mean smoothness',DoubleType(),False),\n","    StructField('mean compactness',DoubleType(),False),StructField('mean concavity',DoubleType(),False),StructField('mean concave points',DoubleType(),False),\n","    StructField('mean symmetry',DoubleType(),False),StructField('mean fractal dimension',DoubleType(),False),StructField('radius error',DoubleType(),False),\n","    StructField('texture error',DoubleType(),False),StructField('perimeter error',DoubleType(),False),StructField('area error',DoubleType(),False),\n","    StructField('smoothness error',DoubleType(),False),StructField('compactness error',DoubleType(),False),StructField('concavity error',DoubleType(),False),\n","    StructField('concave points error',DoubleType(),False),StructField('symmetry error',DoubleType(),False),StructField('fractal dimension error',DoubleType(),False),\n","    StructField('worst radius',DoubleType(),False),StructField('worst texture',DoubleType(),False),StructField('worst perimeter',DoubleType(),False),\n","    StructField('worst area',DoubleType(),False),StructField('worst smoothness',DoubleType(),False),StructField('worst compactness',DoubleType(),False),\n","    StructField('worst concavity',DoubleType(),False),StructField('worst concave points',DoubleType(),False),StructField('worst symmetry',DoubleType(),False),\n","    StructField('worst fractal dimension',DoubleType(),False), StructField('Label',DoubleType(),False)\n","    ])\n","\n","#DataFrameの作成\n","\n","#読み込みデータの指定\n","#b_dir='/content/drive/MyDrive/BigData2024/Data/' # パスを修正\n","#Train_file=b_dir+'sklearn_breast_cancerTrain.csv'\n","#Test_file=b_dir+'sklearn_breast_cancerTest.csv'\n","#データ読み込み（csvファイルを読んでくる）\n","#df5TrainData=ss.read.csv(Train_file,header=True,encoding='UTF-8',schema=struct)\n","#df5TestData=ss.read.csv(Test_file,header=True,encoding='UTF-8',schema=struct)\n","\n","#読み込みデータの指定\n","b_dir='/content/gdrive/MyDrive/BigData2024/Data/' # パスを修正: '/content/drive' を '/content/gdrive' に変更\n","Train_file=b_dir+'sklearn_breast_cancerTrain.csv'\n","Test_file=b_dir+'sklearn_breast_cancerTest.csv'\n","#データ読み込み（csvファイルを読んでくる）\n","df5TrainData=ss.read.csv(Train_file,header=True,encoding='UTF-8',schema=struct)\n","df5TestData=ss.read.csv(Test_file,header=True,encoding='UTF-8',schema=struct)\n","\n","#データの確認表示\n","df5TrainData.show(5,truncate=False)\n","\n","#データ分割（TrainingとValidation)\n","#df5TrainData,df5ValData=df5.randomSplit([0.7,0.3],150) #二つ目の値はSeed\n"],"metadata":{"id":"FpFMM__iOSOY","colab":{"base_uri":"https://localhost:8080/","height":367},"executionInfo":{"status":"error","timestamp":1733972588264,"user_tz":-540,"elapsed":346,"user":{"displayName":"Utaya TAKAIE","userId":"10278607304609517554"}},"outputId":"bccb3beb-211f-4133-e365-d6a40518539c"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AnalysisException","evalue":"[PATH_NOT_FOUND] Path does not exist: file:/content/gdrive/MyDrive/BigData2024/Data/sklearn_breast_cancerTrain.csv.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-deb7fbd1ca26>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mTest_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'sklearn_breast_cancerTest.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#データ読み込み（csvファイルを読んでくる）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mdf5TrainData\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mdf5TestData\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTest_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/gdrive/MyDrive/BigData2024/Data/sklearn_breast_cancerTrain.csv."]}]},{"cell_type":"markdown","source":["# 機械学習（決定木, SVM，多層NN他）"],"metadata":{"id":"C4CRH7sfOMme"}},{"cell_type":"code","source":["from pyspark.ml.classification import DecisionTreeClassifier\n","from pyspark.ml.classification import LinearSVC\n","from pyspark.ml.classification import MultilayerPerceptronClassifier\n","from pyspark.ml.classification import RandomForestClassifier\n","\n","from pyspark.ml.evaluation import  BinaryClassificationEvaluator\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score\n","from pyspark.ml.feature import VectorAssembler\n","\n","#from pyspark.ml.pipeline import Pipeline\n","#from pyspark.ml.tuning import ParamGridBuilder\n","#from pyspark.ml.tuning import CrossValidator"],"metadata":{"id":"wrxbt-qXXr7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#入力データの設定\n","assemblerForCLF=VectorAssembler(inputCols=df5TrainData.columns[1:31],outputCol=\"features\")\n","Train=assemblerForCLF.transform(df5TrainData).select(['features','Label'])\n","Test=assemblerForCLF.transform(df5TestData).select(['features','Label'])\n","\n","#各モデルへの入力特徴指定\n","DTModel=DecisionTreeClassifier(labelCol=\"Label\",featuresCol=\"features\", maxDepth=5)\n","SVCModel=LinearSVC(labelCol=\"Label\",featuresCol=\"features\")\n","\n","layers=[30,20,20,2]\n","MLPModel=MultilayerPerceptronClassifier(labelCol=\"Label\", featuresCol=\"features\",maxIter=100, layers=layers)\n","\n","RFModel=RandomForestClassifier(labelCol=\"Label\",featuresCol=\"features\",maxDepth=5)\n","\n","#各モデルの学習（For文内で実施しているため，コメントアウト）\n","#DT=DTModel.fit(Train)\n","#SVM=SVCModel.fit(Train)\n","#MLP=MLPModel.fit(Train)\n","#RF=RFModel.fit(Train)\n","\n","#学習データに対する評価（For文内で実施しているため，コメントアウト）\n","#predictionByDTC=DT.transform(Train)\n","#predictionBySVM=SVM.transform(Train)\n","#predictionByMLP=MLP.transform(Train)\n","#predictionByRF=RF.transform(Train)\n","\n","\n","#以下はまとめて評価するため\n","Models=[[DTModel,\"Decision Tree\"],[SVCModel,\"SupportVectorMachine\"],[MLPModel,\"MultiLayerPerceptron\"],[RFModel,\"RandomForest\"]]\n","for model,name in Models:\n","  evaluator=BinaryClassificationEvaluator().setLabelCol(model.getLabelCol()).setRawPredictionCol(model.getRawPredictionCol()).setMetricName(\"areaUnderROC\")\n","  Mevaluator=MulticlassClassificationEvaluator().setLabelCol(model.getLabelCol()).setMetricName(\"accuracy\")\n","  TrainedModel=model.fit(Train)\n","  for dtype,ntype in [[Train,\"Train\"],[Test,\"Test\"]]:\n","    prediction=TrainedModel.transform(dtype)\n","    auc=evaluator.evaluate(prediction)\n","    y_pred=prediction.select(\"prediction\").collect()\n","    y_orig=prediction.select(\"label\").collect()\n","    cm=confusion_matrix(y_orig,y_pred)\n","    acc=Mevaluator.evaluate(prediction)\n","    print(\"***Accuracy-\",ntype,\"(Data(\", name, \" ) :\", acc)\n","    #print(accuracy_score(y_orig,y_pred))\n","    print(\"ConfusionMatrix:\\n \",cm)\n","\n","  print(\"\")\n","print(\"\")"],"metadata":{"id":"QCYLHtrAZMm1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672018743701,"user_tz":-540,"elapsed":42386,"user":{"displayName":"村松大吾","userId":"00313224612411644440"}},"outputId":"cf021e05-c287-4704-f07b-e84b0756aee0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["***Accuracy- Train (Data( Decision Tree  ) : 0.9975\n","ConfusionMatrix:\n","  [[141   1]\n"," [  0 258]]\n","***Accuracy- Test (Data( Decision Tree  ) : 0.9526627218934911\n","ConfusionMatrix:\n","  [[64  6]\n"," [ 2 97]]\n","\n","***Accuracy- Train (Data( SupportVectorMachine  ) : 0.995\n","ConfusionMatrix:\n","  [[140   2]\n"," [  0 258]]\n","***Accuracy- Test (Data( SupportVectorMachine  ) : 0.9763313609467456\n","ConfusionMatrix:\n","  [[66  4]\n"," [ 0 99]]\n","\n","***Accuracy- Train (Data( MultiLayerPerceptron  ) : 0.9425\n","ConfusionMatrix:\n","  [[124  18]\n"," [  5 253]]\n","***Accuracy- Test (Data( MultiLayerPerceptron  ) : 0.9467455621301775\n","ConfusionMatrix:\n","  [[62  8]\n"," [ 1 98]]\n","\n","***Accuracy- Train (Data( RandomForest  ) : 0.9975\n","ConfusionMatrix:\n","  [[141   1]\n"," [  0 258]]\n","***Accuracy- Test (Data( RandomForest  ) : 0.9704142011834319\n","ConfusionMatrix:\n","  [[66  4]\n"," [ 1 98]]\n","\n","\n"]}]},{"cell_type":"markdown","source":["# 設定を変更して実験をしてみよう\n"],"metadata":{"id":"CRgJtWGR2ixG"}},{"cell_type":"code","source":["#入力データの設定\n","\n","#利用次元の設定（各自で変えて実行してみよう）\n","\n","selectFeatures=df5TrainData.columns[11:21]\n","#selectFeatures=[\"mean radius\",\"mean perimeter\",\"mean compactness\"] #,\"mean symmetry\",\"worst radius\",\"worst perimeter\",\"worst compactness\",\"worst symmetry\"]\n","\n","assemblerForCLF=VectorAssembler(inputCols=selectFeatures,outputCol=\"features\")\n","Train=assemblerForCLF.transform(df5TrainData).select(['features','Label'])\n","Test=assemblerForCLF.transform(df5TestData).select(['features','Label'])\n","\n","\n","#各モデルへの入力特徴指定\n","DTModel=DecisionTreeClassifier(labelCol=\"Label\",featuresCol=\"features\", maxBins=32, maxDepth=5)\n","SVCModel=LinearSVC(labelCol=\"Label\",featuresCol=\"features\",maxIter=150)\n","\n","#MultiLayerPerceptronのネットワーク構造\n","layers=[len(selectFeatures),30,20,10,2]\n","\n","MLPModel=MultilayerPerceptronClassifier(labelCol=\"Label\", featuresCol=\"features\",maxIter=100, layers=layers)\n","RFModel=RandomForestClassifier(labelCol=\"Label\",featuresCol=\"features\",maxBins=32)\n","\n","\n","Models=[[DTModel,\"Decision Tree\"],[SVCModel,\"SupportVectorMachine\"],[MLPModel,\"MultiLayerPerceptron\"],[RFModel,\"RandomForest\"]]\n","\n","for model,name in Models:\n","  evaluator=BinaryClassificationEvaluator().setLabelCol(model.getLabelCol()).setRawPredictionCol(model.getRawPredictionCol()).setMetricName(\"areaUnderROC\")\n","  Mevaluator=MulticlassClassificationEvaluator().setLabelCol(model.getLabelCol()).setMetricName(\"accuracy\")\n","  TrainedModel=model.fit(Train)\n","  for dtype,ntype in [[Train,\"Train\"],[Test,\"Test\"]]:\n","    prediction=TrainedModel.transform(dtype)\n","    auc=evaluator.evaluate(prediction)\n","    y_pred=prediction.select(\"prediction\").collect()\n","    y_orig=prediction.select(\"label\").collect()\n","    cm=confusion_matrix(y_orig,y_pred)\n","    acc=Mevaluator.evaluate(prediction)\n","    print(\"***Accuracy-\",ntype,\"(Data(\", name, \" ) :\", acc)\n","    print(\"ConfusionMatrix:\\n \",cm)\n","\n","  print(\"\")\n","print(\"\")\n"],"metadata":{"id":"fN7Nqd_-R8uX"},"execution_count":null,"outputs":[]}]}